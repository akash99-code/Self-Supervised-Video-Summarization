{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYrK8vrOtv9l",
        "outputId": "f859444f-8173-4df8-ff1d-8c2d8248c94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/coot-videotext-master/NEW"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqiRnc3zuLWZ",
        "outputId": "eed4e57d-330a-46a0-eff4-674db5d54733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/coot-videotext-master/NEW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXBGvHhyuOFJ",
        "outputId": "a325b178-8676-4653-db03-4686f5511eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mconfig\u001b[0m/            meta_all.json   Model_training.ipynb  \u001b[01;34mprovided_models\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/              metaSUMMe.json  \u001b[01;34mnntrainer\u001b[0m/            summaries.h5\n",
            "\u001b[01;34mexperiments\u001b[0m/       metaTVsum.json  pre-processing.ipynb\n",
            "frms_per_seg.json  \u001b[01;34mmodels\u001b[0m/         \u001b[01;34mpretrained_models\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import h5py\n",
        "import math\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "kB877GSLuh5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install GPUtil\n",
        "! pip install pathspec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ymRaODAuwKY",
        "outputId": "ef345d62-18f9-46bf-c636-8cec36bdabd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: GPUtil in /usr/local/lib/python3.9/dist-packages (1.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pathspec in /usr/local/lib/python3.9/dist-packages (0.11.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nntrainer.models.transformer_legacy import TransformerLegacy\n",
        "from nntrainer.models.transformer_legacy import TransformerConfig"
      ],
      "metadata": {
        "id": "zUxTzNdxukDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transf_config = {'name': 'transformer', 'output_dim': 384, 'use_input_fc': True, 'input_fc_config': {'output_dim': 384, 'num_layers': 1, 'hidden_dim': 0, 'activation_middle': 'none', 'activation_output': 'gelu', 'dropout_middle': 0, 'dropout_output': 0, 'norm_middle': 'none', 'norm_output': 'none', 'residual': 'none'}, 'positional_encoding': 'sincos', 'add_local_cls_token': False, 'dropout_input': 0, 'norm_input': 'layernorm_coot', 'selfatn_config': {'hidden_dim': 384, 'num_layers': 1, 'num_heads': 8, 'pointwise_ff_dim': 384, 'activation': 'gelu', 'dropout': 0.05, 'norm': 'layernorm_coot'}, 'use_context': False, 'use_output_fc': False, 'pooler_config': {'name': 'atn', 'hidden_dim': 768, 'num_heads': 2, 'num_layers': 1, 'dropout': 0.05, 'activation': 'gelu'}, 'weight_init_type': 'truncnorm', 'weight_init_std': 0.01}\n",
        "current_cfg = TransformerConfig(transf_config)\n",
        "net_video_local = TransformerLegacy(current_cfg, 512)"
      ],
      "metadata": {
        "id": "zoxcYga8urXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_video_local.load_state_dict(torch.load('provided_models/Net_local.pth'))\n",
        "net_video_local.eval()\n",
        "net_video_local.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ9wkMSWvA9J",
        "outputId": "866550d5-93dc-42e3-bfa5-b1c3fe39828c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerLegacy(\n",
              "  (norm_input): LayerNormalization()\n",
              "  (input_fc): MLP(\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=384, bias=True)\n",
              "    )\n",
              "    (activation_output): GELU(approximate='none')\n",
              "    (norm_output): Identity()\n",
              "  )\n",
              "  (embedding): PositionalEncodingSinCos(\n",
              "    (dropout): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (tf): TransformerEncoder(\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attention_layer): Sublayer(\n",
              "          (sublayer): MultiHeadAttention(\n",
              "            (query_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (key_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (value_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (final_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.05, inplace=False)\n",
              "            (softmax): Softmax(dim=3)\n",
              "          )\n",
              "          (layer_normalization): LayerNormalization()\n",
              "        )\n",
              "        (pointwise_feedforward_layer): Sublayer(\n",
              "          (sublayer): PointwiseFeedForwardNetwork(\n",
              "            (feed_forward): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (1): Dropout(p=0.05, inplace=False)\n",
              "              (2): GELU(approximate='none')\n",
              "              (3): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (4): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (layer_normalization): LayerNormalization()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.05, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): MultiGenPool(\n",
              "    (pools): ModuleList(\n",
              "      (0): GenPool(\n",
              "        pool linear torch.Size([2, 384, 384])\n",
              "        pool linear torch.Size([2, 384])\n",
              "        pool linear torch.Size([2, 384, 192])\n",
              "        pool linear torch.Size([2, 192])\n",
              "        (activation): GELU(approximate='none')\n",
              "        (dropout1): Dropout(p=0.05, inplace=False)\n",
              "        (dropout2): Dropout(p=0.05, inplace=False)\n",
              "        (dropout3): Dropout(p=0.05, inplace=False)\n",
              "        (softmax): Softmax(dim=2)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transf_config = {'name': 'transformer', 'output_dim': 768, 'use_input_fc': False, 'input_fc_config': None, 'use_context': True, 'crossatn_config': {'hidden_dim': 384, 'num_layers': 1, 'num_heads': 8, 'pointwise_ff_dim': 384, 'activation': 'gelu', 'dropout': 0.05, 'norm': 'layernorm_coot'}, 'pooler_config': {'name': 'avg_special'}, 'positional_encoding': 'sincos', 'add_local_cls_token': False, 'dropout_input': 0, 'norm_input': 'layernorm_coot', 'selfatn_config': {'hidden_dim': 384, 'num_layers': 1, 'num_heads': 8, 'pointwise_ff_dim': 384, 'activation': 'gelu', 'dropout': 0.05, 'norm': 'layernorm_coot'}, 'use_output_fc': False, 'weight_init_type': 'truncnorm', 'weight_init_std': 0.01}\n",
        "current_cfg = TransformerConfig(transf_config)\n",
        "coot = TransformerLegacy(current_cfg, 384)"
      ],
      "metadata": {
        "id": "3TNNjhnFvDFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coot.load_state_dict(torch.load('provided_models/TheModel.pth'))\n",
        "coot.eval()\n",
        "coot.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLgSsfBRvEiA",
        "outputId": "1ce6e95b-55cd-4280-f72e-2a98e5c9cee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerLegacy(\n",
              "  (norm_input): LayerNormalization()\n",
              "  (embedding): PositionalEncodingSinCos(\n",
              "    (dropout): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (tf): TransformerEncoder(\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attention_layer): Sublayer(\n",
              "          (sublayer): MultiHeadAttention(\n",
              "            (query_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (key_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (value_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (final_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.05, inplace=False)\n",
              "            (softmax): Softmax(dim=3)\n",
              "          )\n",
              "          (layer_normalization): LayerNormalization()\n",
              "        )\n",
              "        (pointwise_feedforward_layer): Sublayer(\n",
              "          (sublayer): PointwiseFeedForwardNetwork(\n",
              "            (feed_forward): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (1): Dropout(p=0.05, inplace=False)\n",
              "              (2): GELU(approximate='none')\n",
              "              (3): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (4): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (layer_normalization): LayerNormalization()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.05, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (tf_context): TransformerDecoder(\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attention_layer): Sublayer(\n",
              "          (sublayer): MultiHeadAttention(\n",
              "            (query_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (key_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (value_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (final_projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.05, inplace=False)\n",
              "            (softmax): Softmax(dim=3)\n",
              "          )\n",
              "          (layer_normalization): LayerNormalization()\n",
              "        )\n",
              "        (pointwise_feedforward_layer): Sublayer(\n",
              "          (sublayer): PointwiseFeedForwardNetwork(\n",
              "            (feed_forward): Sequential(\n",
              "              (0): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (1): Dropout(p=0.05, inplace=False)\n",
              "              (2): GELU(approximate='none')\n",
              "              (3): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (4): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (layer_normalization): LayerNormalization()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.05, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): TemporalAvgPool()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarizer model"
      ],
      "metadata": {
        "id": "lkmdoUEDvGyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knapsack_dp(values,weights,n_items,capacity,device='cpu'):\n",
        "    # check_inputs(values,weights,n_items,capacity)\n",
        "    \n",
        "    # assert(isinstance(values,list))\n",
        "    # assert(isinstance(weights,list))\n",
        "    # assert(isinstance(n_items,int))\n",
        "    # assert(isinstance(capacity,int))\n",
        "    # check value type\n",
        "    # assert(all(isinstance(val,int) or isinstance(val,float) for val in values))\n",
        "    # assert(all(isinstance(val,int) for val in weights))\n",
        "    # check validity of value\n",
        "    # assert(all(val >= 0 for val in weights))\n",
        "    # assert(n_items > 0)\n",
        "    # assert(capacity > 0)\n",
        "\n",
        "    table = torch.zeros((n_items+1,capacity+1)).to(device)\n",
        "    keep = torch.zeros((n_items+1,capacity+1)).to(device)\n",
        "\n",
        "    for i in range(1,n_items+1):\n",
        "        for w in range(0,capacity+1):\n",
        "            wi = weights[i-1] # weight of current item\n",
        "            vi = values[i-1] # value of current item\n",
        "            if (wi <= w) and (vi + table[i-1,w-wi] > table[i-1,w]):\n",
        "                table[i,w] = vi*1 + table[i-1,w-wi]\n",
        "                keep[i,w] = 1\n",
        "            else:\n",
        "                table[i,w] = vi*0+table[i-1,w]\n",
        "    \n",
        "    \n",
        "    picks = []\n",
        "    cum_wghts = torch.zeros(n_items).to(device)\n",
        "    K = capacity\n",
        "    for i in range(n_items,0,-1):\n",
        "        if keep[i,K] == 1:\n",
        "            picks.append(i)\n",
        "            cum_wghts[i-1] = table[i,K]\n",
        "            K -= weights[i-1]\n",
        "\n",
        "    picks.sort()\n",
        "    picks = [x-1 for x in picks] # change to 0-index\n",
        "    chosens = (cum_wghts-torch.min(cum_wghts)) / (torch.max(cum_wghts)-torch.min(cum_wghts))\n",
        "    # print(picks, torch.ceil(chosens) )\n",
        "    \n",
        "\n",
        "    # if return_all:\n",
        "    #     max_val = table[n_items,capacity]\n",
        "    #     return picks,max_val\n",
        "    return picks, torch.ceil(chosens)"
      ],
      "metadata": {
        "id": "XAEno2RLvGD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShotScore(nn.Module):\n",
        "    def __init__(self, input_size=384):\n",
        "        super(ShotScore, self).__init__()\n",
        "        self.Clip_Encoding = nn.Sequential(\n",
        "                                nn.TransformerEncoderLayer(d_model=input_size, nhead=8, batch_first=True, activation = 'gelu'),\n",
        "                                nn.TransformerEncoderLayer(d_model=input_size, nhead=8, batch_first=True, activation = 'gelu'))\n",
        "        self.Clip_Scoring = nn.Sequential(\n",
        "                                nn.Linear(in_features=input_size, out_features=input_size//2, bias=True),\n",
        "                                nn.Linear(in_features=input_size//2, out_features=1, bias=True))\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "    \n",
        "    \n",
        "    def forward(self,clip_embs, stages,  batch_mask=None):\n",
        "        \n",
        "        f1 = clip_embs\n",
        "#         for s in range(stages):\n",
        "#             clip_encoder = self.Clip_Encoding(Features)+Features\n",
        "#             clip_Scores = self.Clip_Scoring(clip_encoder)\n",
        "\n",
        "#     #         Scene_emb_list = []\n",
        "#     #         for start, end in scene_boundaries:\n",
        "#     #             scene_embs = self.scene_embs(clip_embs[start:end])\n",
        "#     #             Scene_emb_list.append(scene_embs)\n",
        "\n",
        "#     #             Scene_Scores = self.Scene_Scoring_Transformer(Scene_emb_list)\n",
        "\n",
        "#     #         for start, end in scene_boundaries:  \n",
        "#                 # clip_Scores+=Scene_Scores\n",
        "\n",
        "#             si = self.softmax(clip_Scores)\n",
        "#             progressive_scores.append(si)\n",
        "#             Features += si*Features\n",
        "        \n",
        "        s1 = self.softmax(self.Clip_Scoring(self.Clip_Encoding(f1)+f1))\n",
        "        \n",
        "        f2 = s1*f1 + f1\n",
        "        s2 = self.softmax(self.Clip_Scoring(self.Clip_Encoding(f2)+f2))\n",
        "        \n",
        "        f3 = s2*f2 + f2\n",
        "        s3 = self.softmax(self.Clip_Scoring(self.Clip_Encoding(f3)+f3))\n",
        "            \n",
        "        \n",
        "        scores = self.softmax(s1)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "ECN83s8_vJkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, apperture=-1, ignore_itself=False, input_size=1024, output_size=1024):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.apperture = apperture\n",
        "        self.ignore_itself = ignore_itself\n",
        "\n",
        "        self.m = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.K = nn.Linear(in_features=self.m, out_features=self.output_size, bias=False)\n",
        "        self.Q = nn.Linear(in_features=self.m, out_features=self.output_size, bias=False)\n",
        "        self.V = nn.Linear(in_features=self.m, out_features=self.output_size, bias=False)\n",
        "        self.output_linear = nn.Linear(in_features=self.output_size, out_features=self.m, bias=False)\n",
        "\n",
        "        self.drop50 = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n = x.shape[0]  # sequence length\n",
        "\n",
        "        K = self.K(x)  # ENC (n x m) => (n x H) H= hidden size\n",
        "        Q = self.Q(x)  # ENC (n x m) => (n x H) H= hidden size\n",
        "        V = self.V(x)\n",
        "\n",
        "        Q *= 0.06\n",
        "        logits = torch.matmul(Q, K.transpose(1,0))\n",
        "\n",
        "        if self.ignore_itself:\n",
        "            # Zero the diagonal activations (a distance of each frame with itself)\n",
        "            logits[torch.eye(n).byte()] = -float(\"Inf\")\n",
        "\n",
        "        if self.apperture > 0:\n",
        "            # Set attention to zero to frames further than +/- apperture from the current one\n",
        "            onesmask = torch.ones(n, n)\n",
        "            trimask = torch.tril(onesmask, -self.apperture) + torch.triu(onesmask, self.apperture)\n",
        "            logits[trimask == 1] = -float(\"Inf\")\n",
        "\n",
        "        att_weights_ = nn.functional.softmax(logits, dim=-1)\n",
        "        weights = self.drop50(att_weights_)\n",
        "        y = torch.matmul(V.transpose(1,0), weights).transpose(1,0)\n",
        "        y = self.output_linear(y)\n",
        "\n",
        "        return y, att_weights_\n",
        "\n",
        "\n",
        "class VASNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VASNet, self).__init__()\n",
        "\n",
        "        self.m = 384 # cnn features size\n",
        "        self.hidden_size = 384\n",
        "\n",
        "        self.att = SelfAttention(input_size=self.m, output_size=self.m)\n",
        "        self.ka = nn.Linear(in_features=self.m, out_features=384)\n",
        "        self.kb = nn.Linear(in_features=self.ka.out_features, out_features=384)\n",
        "        self.kc = nn.Linear(in_features=self.kb.out_features, out_features=384)\n",
        "        self.kd = nn.Linear(in_features=self.ka.out_features, out_features=1)\n",
        "\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.relu = nn.GELU()\n",
        "        self.drop50 = nn.Dropout(0.5)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.layer_norm_y = LayerNorm(self.m)\n",
        "        self.layer_norm_ka = LayerNorm(self.ka.out_features)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        m = x.shape[2] # Feature size\n",
        "\n",
        "        # Place the video frames to the batch dimension to allow for batch arithm. operations.\n",
        "        # Assumes input batch size = 1.\n",
        "        x = x.view(-1, m)\n",
        "        y, att_weights_ = self.att(x)\n",
        "\n",
        "        y = y + x\n",
        "        y = self.drop50(y)\n",
        "        y = self.layer_norm_y(y)\n",
        "\n",
        "        # Frame level importance score regression\n",
        "        # Two layer NN\n",
        "        y = self.ka(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.drop50(y)\n",
        "        y = self.layer_norm_ka(y)\n",
        "\n",
        "        # y = self.kd(y)\n",
        "        # y = self.sig(y)\n",
        "        # y = y.view(1, -1)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class ShotScore2(nn.Module):\n",
        "    def __init__(self, input_size=384):\n",
        "        super(ShotScore2, self).__init__()\n",
        "        self.Clip_Encoding = nn.Sequential(\n",
        "                                nn.TransformerEncoderLayer(d_model=input_size, nhead=8, batch_first=True, activation = 'gelu'),\n",
        "                                nn.TransformerEncoderLayer(d_model=input_size, nhead=8, batch_first=True, activation = 'gelu'))\n",
        "        self.Clip_Scoring = nn.Sequential(\n",
        "                                nn.Linear(in_features=input_size, out_features=input_size//2, bias=True),\n",
        "                                nn.Linear(in_features=input_size//2, out_features=1, bias=True))\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.vasnet = VASNet()\n",
        "        \n",
        "    \n",
        "    \n",
        "    def forward(self,clip_embs, stages,  batch_mask=None):\n",
        "        \n",
        "        f1 = clip_embs\n",
        "#         for s in range(stages):\n",
        "#             clip_encoder = self.Clip_Encoding(Features)+Features\n",
        "#             clip_Scores = self.Clip_Scoring(clip_encoder)\n",
        "\n",
        "#     #         Scene_emb_list = []\n",
        "#     #         for start, end in scene_boundaries:\n",
        "#     #             scene_embs = self.scene_embs(clip_embs[start:end])\n",
        "#     #             Scene_emb_list.append(scene_embs)\n",
        "\n",
        "#     #             Scene_Scores = self.Scene_Scoring_Transformer(Scene_emb_list)\n",
        "\n",
        "#     #         for start, end in scene_boundaries:  \n",
        "#                 # clip_Scores+=Scene_Scores\n",
        "\n",
        "#             si = self.softmax(clip_Scores)\n",
        "#             progressive_scores.append(si)\n",
        "#             Features += si*Features\n",
        "        \n",
        "        s1 = self.softmax(self.Clip_Scoring(self.vasnet(f1)+f1))\n",
        "        \n",
        "        \n",
        "        # f2 = s1*f1 + f1\n",
        "        # s2 = self.softmax(self.Clip_Scoring(self.vasnet(f2)+f2))\n",
        "        \n",
        "        \n",
        "        # f3 = s2*f2 + f2\n",
        "        # s3 = self.softmax(self.Clip_Scoring(self.vasnet(f3)+f3))\n",
        "            \n",
        "        scores = self.softmax(s1)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "oXC2KWb5pUWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer"
      ],
      "metadata": {
        "id": "hQ1eXqGtvMI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname == 'Linear':\n",
        "        torch.nn.init.xavier_uniform_(m.weight, gain=np.sqrt(2.0))\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0.1)"
      ],
      "metadata": {
        "id": "OvAehabNvLGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hidden_context(key, pred_clip_mask, nfps, device='cpu'):\n",
        "    with h5py.File('data/youcook2/video_feat_100m.h5', 'r') as d:\n",
        "        vid_feats = torch.tensor(d[key][()]).to(device)\n",
        "        mask = torch.zeros(len(vid_feats)).to(device)\n",
        "        \n",
        "        # start = 0\n",
        "        # for i in range(len(nfps)):\n",
        "        #     end = nfps[i]+start\n",
        "        #     if i in picks:\n",
        "        #         mask[start:end]=1\n",
        "        #     start = end\n",
        "        \n",
        "        start = 0\n",
        "        for i in range(len(nfps)):\n",
        "            end = nfps[i]+start\n",
        "            mask[start:end]=pred_clip_mask[i]\n",
        "            start = end\n",
        "            \n",
        "\n",
        "        masked_vidfeats = vid_feats*mask.unsqueeze(0).T\n",
        "        masked_vidfeats = masked_vidfeats.unsqueeze(0)\n",
        "\n",
        "        # print(masked_vidfeats, vid_feats, mask)\n",
        "        # masked_vidfeats = masked_vidfeats[masked_vidfeats.sum(dim=2) != 0].unsqueeze(0)\n",
        "        # nfs = mask.sum().int()\n",
        "        nfs = len(vid_feats)\n",
        "        mask = torch.zeros(1, nfs).bool().to(device)\n",
        "        vid_context,_ = net_video_local(masked_vidfeats, mask, torch.tensor(nfs).int(), None)\n",
        "        return vid_context\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "bUd-ctg4vOyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sim = torch.nn.CosineSimilarity(dim=0, eps=0)\n",
        "def diversity_loss(masked_vid):\n",
        "    agg = []\n",
        "    dl = 0\n",
        "    for i in range(masked_vid.shape[0]):\n",
        "        for j in range(i+1, masked_vid.shape[0]):\n",
        "            yi = masked_vid[i]\n",
        "            yj = masked_vid[j]\n",
        "            cs = cos_sim(yi, yj)\n",
        "            agg.append(cs)\n",
        "            dl+=cs\n",
        "            \n",
        "    return torch.mean(torch.stack(agg))\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "0C3lFKVOvQBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Summe = ['Base jumping', 'Bearpark_climbing', 'Bearpark_climbing', 'Bus_in_Rock_Tunnel', 'car_over_camera', 'Car_railcrossing', 'Cockpit_Landing',  'Cooking', 'Eiffel Tower', 'Excavators river crossing', 'Fire Domino', 'Jumps', 'Kids_playing_in_leaves','Notre_Dame', 'Paintball', 'paluma_jump', 'playing_ball', 'Playing_on_water_slide', 'Saving dolphines', 'Scuba', 'St Maarten Landing', 'Statue of Liberty', 'Uncut_Evening_Flight','Valparaiso_Downhill', 'Air_Force_One']\n",
        "len(Summe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgkaFK-CTmB_",
        "outputId": "b3c68aaa-b33b-4416-b934-18cf5bdece35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, batch_size=1, device='cpu'):\n",
        "    with open('frms_per_seg.json', 'r') as file:\n",
        "        NFPS = json.load(file)\n",
        "    l = nn.MSELoss().to(device)\n",
        "    elu = nn.ELU()\n",
        "    ScoreModel = model\n",
        "    parameters = filter(lambda p: p.requires_grad, ScoreModel.parameters())\n",
        "    optimizer = torch.optim.AdamW(parameters)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
        "    prev_mask=0\n",
        "    batch_loss = []\n",
        "    with h5py.File('experiments/retrieval/paper2020/yc2_100m_coot_valset1/embeddings/embeddings_0.h5', 'r') as d:\n",
        "        clips = d['clip_num'][...]\n",
        "        \n",
        "        for epoch in range(50):\n",
        "            pointer = 0\n",
        "            epoch_loss = []\n",
        "            \n",
        "            print('Epoch:', epoch,'/50')\n",
        "            pbar = tqdm(total=len(clips), position=0, leave=True)\n",
        "            for clip_num, nclips in enumerate(clips):\n",
        "                key = d['key'][clip_num].decode()\n",
        "                \n",
        "                clip_embs = d['clip_emb'][pointer:pointer+nclips]\n",
        "                ori_vid_cont = d['vid_context'][clip_num]\n",
        "                ori_vid_emb = d['vid_emb'][clip_num]\n",
        "                pointer += nclips\n",
        "                \n",
        "                if key in Summe:\n",
        "                  continue\n",
        "\n",
        "                # vid_cont = torch.tensor(vid_context).unsqueeze(0)\n",
        "                clip_embs = torch.tensor(clip_embs).unsqueeze(0).to(device)\n",
        "                ori_vid_cont = torch.tensor(ori_vid_cont).unsqueeze(0).to(device)\n",
        "                ori_vid_emb = torch.tensor(ori_vid_emb).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "                scores = ScoreModel(clip_embs,3).reshape(-1)\n",
        "\n",
        "                nfps = NFPS[key]\n",
        "\n",
        "                picks, predicted_mask = knapsack_dp(scores.squeeze(0), NFPS[key], len(nfps), math.ceil(sum(nfps)*15/100), device=device)\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "                # print('###')\n",
        "                # predicted_mask = torch.zeros(nclips)\n",
        "                # predicted_mask[picks]=1\n",
        "\n",
        "                # mask = torch.ceil(predicted_mask.unsqueeze(0)*scores)\n",
        "                maskr = predicted_mask.unsqueeze(0)\n",
        "                maskc = predicted_mask-1\n",
        "\n",
        "                s1 = torch.ceil(scores*maskr)\n",
        "\n",
        "                s2 = elu(scores*maskc)\n",
        "                mask = torch.abs(s1+s2)\n",
        "\n",
        "                vid_cont = get_hidden_context(key, mask.squeeze(), nfps, device)\n",
        "\n",
        "\n",
        "                # if clip_num==0:\n",
        "                #     print(mask-prev_mask)\n",
        "                #     prev_mask=mask\n",
        "                    \n",
        "                masked_vid = clip_embs*mask.T\n",
        "\n",
        "                # masked_vid = masked_vid[masked_vid.sum(dim=2) != 0].unsqueeze(0)\n",
        "\n",
        "                ### Check masked_vid has grads\n",
        "                # clipn = predicted_mask.sum().int()\n",
        "                clipn = nclips\n",
        "                cootmask = torch.zeros(1, clipn).bool().to(device)\n",
        "                \n",
        "                vid_emb, _ = coot(masked_vid, cootmask, torch.tensor(clipn).int().to(device), vid_cont)\n",
        "                # print(ori_vid_emb.shape)\n",
        "                # print(vid_emb.shape)\n",
        "\n",
        "                selected_clips = masked_vid*predicted_mask.unsqueeze(0).T\n",
        "                selected_clips = selected_clips[selected_clips.sum(dim=2) != 0].unsqueeze(0)\n",
        "\n",
        "                div_loss = diversity_loss(selected_clips.squeeze())\n",
        "                reconstruction_loss = l(vid_emb, ori_vid_emb)\n",
        "                context_loss  = l(vid_cont, ori_vid_cont)\n",
        "\n",
        "                Loss = (0.1*div_loss) + (0.9*context_loss)\n",
        "\n",
        "                batch_loss.append(Loss.unsqueeze(0)) \n",
        "\n",
        "                if(len(batch_loss) == 5):\n",
        "                  BLoss = torch.mean(torch.cat(batch_loss))\n",
        "                  optimizer.zero_grad()\n",
        "                  BLoss.backward()\n",
        "                  optimizer.step()\n",
        "                  scheduler.step(BLoss)\n",
        "                  batch_loss = []\n",
        "\n",
        "            #     optimizer.zero_grad()\n",
        "            #     Loss.backward()\n",
        "            #     optimizer.step()\n",
        "\n",
        "                epoch_loss.append(Loss.item())\n",
        "                \n",
        "                pbar.update(1)\n",
        "                pbar.set_description(\"Loss= %.3f\" % Loss.item())\n",
        "            pbar.close()\n",
        "                \n",
        "            print('Average Epoch Loss:', sum(epoch_loss)/len(epoch_loss))\n",
        "            torch.save(ScoreModel.state_dict(), f'models/model@{epoch%5}.pth')\n",
        "            \n",
        "            if(epoch%1==0):\n",
        "                fs_gt, fs_us = validation(ScoreModel, device='cuda')\n",
        "                print('Avg GT FSore:',fs_gt, '   Avg User FSore:', fs_us)\n",
        "            \n",
        "            \n",
        "    return(ScoreModel)\n",
        "    \n",
        "            "
      ],
      "metadata": {
        "id": "BxLdYaJ-vRWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "57E54b43vUPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(ScoreModel, device='cpu'):\n",
        "    \n",
        "    with open('frms_per_seg.json', 'r') as file:\n",
        "        NFPS = json.load(file)\n",
        "        \n",
        "    avg_gtfs = []\n",
        "    avg_usfs = []\n",
        "    with h5py.File('experiments/retrieval/paper2020/yc2_100m_coot_valset1/embeddings/embeddings_0.h5', 'r') as d, h5py.File('summaries.h5','r') as summh5:\n",
        "        clips = d['clip_num'][...]\n",
        "\n",
        "        pointer = 0\n",
        "        for clip_num, nclips in enumerate(clips):\n",
        "            key = d['key'][clip_num].decode()\n",
        "            if key in Summe:\n",
        "              continue\n",
        "            clip_embs = d['clip_emb'][pointer:pointer+nclips]\n",
        "            pointer += nclips\n",
        "\n",
        "            clip_embs = torch.tensor(clip_embs).unsqueeze(0).to(device)\n",
        "\n",
        "            scores = ScoreModel(clip_embs, 3).reshape(-1).cpu()\n",
        "\n",
        "            nfps = NFPS[key]\n",
        "\n",
        "            picks,_ = knapsack_dp(scores.squeeze(0), NFPS[key], len(nfps), math.ceil(sum(nfps)*15/100))\n",
        "            \n",
        "            machine_summary = get_machine_summ(picks, nfps)\n",
        "            gt_summary = summh5[key]['gt_summary']\n",
        "            user_summary = summh5[key]['user_summary']\n",
        "            fs_gt,_,_ = evaluate_summary(machine_summary, np.expand_dims(gt_summary, 0) )\n",
        "            fs_ut,_,_ = evaluate_summary(machine_summary, np.array(user_summary) )\n",
        "            avg_gtfs.append(fs_gt)\n",
        "            avg_usfs.append(avg_usfs)\n",
        "            \n",
        "        return fs_gt.mean(), fs_ut.mean()\n"
      ],
      "metadata": {
        "id": "aioRJ20nvTvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_machine_summ(picks, nfps):\n",
        "    tot_feats = sum(nfps)\n",
        "    summ = []\n",
        "    for n, nfs in enumerate(nfps):\n",
        "        if n in picks:\n",
        "            summ = np.append(summ, np.ones(16*nfs))\n",
        "        else:\n",
        "            summ = np.append(summ, np.zeros(16*nfs))\n",
        "    return summ"
      ],
      "metadata": {
        "id": "duL0m5KyvWWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_summary(machine_summary, user_summary, eval_metric='avg'):\n",
        "    \"\"\"Compare machine summary with user summary (keyshot-based).\n",
        "    Args:\n",
        "    --------------------------------\n",
        "    machine_summary and user_summary should be binary vectors of ndarray type.\n",
        "    eval_metric = {'avg', 'max'}\n",
        "    'avg' averages results of comparing multiple human summaries.\n",
        "    'max' takes the maximum (best) out of multiple comparisons.\n",
        "    \"\"\"\n",
        "    machine_summary = machine_summary.astype(np.float32)\n",
        "    user_summary = user_summary.astype(np.float32)\n",
        "    n_users,n_frames = user_summary.shape\n",
        "\n",
        "    # binarization\n",
        "    machine_summary[machine_summary > 0] = 1\n",
        "    user_summary[user_summary > 0] = 1\n",
        "\n",
        "    if len(machine_summary) > n_frames:\n",
        "        machine_summary = machine_summary[:n_frames]\n",
        "    elif len(machine_summary) < n_frames:\n",
        "        zero_padding = np.zeros((n_frames - len(machine_summary)))\n",
        "        machine_summary = np.concatenate([machine_summary, zero_padding])\n",
        "\n",
        "    f_scores = []\n",
        "    prec_arr = []\n",
        "    rec_arr = []\n",
        "\n",
        "    for user_idx in range(n_users):\n",
        "        gt_summary = user_summary[user_idx,:]\n",
        "        overlap_duration = (machine_summary * gt_summary).sum()\n",
        "        precision = overlap_duration / (machine_summary.sum() + 1e-8)\n",
        "        recall = overlap_duration / (gt_summary.sum() + 1e-8)\n",
        "        if precision == 0 and recall == 0:\n",
        "            f_score = 0.\n",
        "        else:\n",
        "            f_score = (2 * precision * recall) / (precision + recall)\n",
        "        f_scores.append(f_score)\n",
        "        prec_arr.append(precision)\n",
        "        rec_arr.append(recall)\n",
        "\n",
        "    final_f_score = np.mean(f_scores)\n",
        "    final_prec = np.mean(prec_arr)\n",
        "    final_rec = np.mean(rec_arr)\n",
        "    return final_f_score, final_prec, final_rec"
      ],
      "metadata": {
        "id": "qv5rLCyrvXwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "rqrUiHv4vaJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ShotScore2()\n",
        "# model.load_state_dict(torch.load('models/model@1.pth', map_location=lambda storage, loc: storage)) \n",
        "model.apply(weights_init)\n",
        "model.to('cuda')\n",
        "trained_model = train(model, device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KEbEoXRXvZRB",
        "outputId": "1fb392d0-0a64-4047-fbf1-61bba3b4dc17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.600:  67%|██████▋   | 48/72 [01:09<00:34,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Epoch Loss: 0.5745956897735596\n",
            "Avg GT FSore: 0.3489148580910031    Avg User FSore: 0.2851319676361825\n",
            "Epoch: 1 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.598:  67%|██████▋   | 48/72 [01:16<00:38,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Epoch Loss: 0.574719017992417\n",
            "Avg GT FSore: 0.44090528079730246    Avg User FSore: 0.2852468524256314\n",
            "Epoch: 2 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.598:  67%|██████▋   | 48/72 [01:08<00:34,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Epoch Loss: 0.574887772401174\n",
            "Avg GT FSore: 0.36227045074520414    Avg User FSore: 0.27436353127903723\n",
            "Epoch: 3 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.601:  67%|██████▋   | 48/72 [01:14<00:37,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Epoch Loss: 0.5746488931278387\n",
            "Avg GT FSore: 0.3956594323807069    Avg User FSore: 0.3185181505305591\n",
            "Epoch: 4 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.602:  67%|██████▋   | 48/72 [01:11<00:35,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Epoch Loss: 0.5750665441155434\n",
            "Avg GT FSore: 0.44090528079730246    Avg User FSore: 0.3003916646154528\n",
            "Epoch: 5 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.601:  67%|██████▋   | 48/72 [01:09<00:34,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Epoch Loss: 0.575231809169054\n",
            "Avg GT FSore: 0.28380634390177284    Avg User FSore: 0.24868606100559593\n",
            "Epoch: 6 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.599:  67%|██████▋   | 48/72 [01:11<00:35,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Epoch Loss: 0.5751207694411278\n",
            "Avg GT FSore: 0.27712854757467226    Avg User FSore: 0.2203525154348675\n",
            "Epoch: 7 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.601:  67%|██████▋   | 48/72 [01:08<00:34,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Epoch Loss: 0.5749880547324816\n",
            "Avg GT FSore: 0.43739565942508524    Avg User FSore: 0.3376620296134274\n",
            "Epoch: 8 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.598:  67%|██████▋   | 48/72 [01:11<00:35,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Epoch Loss: 0.5748485041161379\n",
            "Avg GT FSore: 0.28714524206532316    Avg User FSore: 0.23829048693498586\n",
            "Epoch: 9 /50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss= 0.564:  54%|█████▍    | 39/72 [00:54<01:08,  2.07s/it]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-5a826e17fae7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-bb4388afc3b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch_size, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mnfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNFPS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mpicks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknapsack_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNFPS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-c778bd7cb36a>\u001b[0m in \u001b[0;36mknapsack_dp\u001b[0;34m(values, weights, n_items, capacity, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# weight of current item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# value of current item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwi\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mkeep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jd8MpdfRYNxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xt0IALMjYNns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QXGXBb_LYNiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFBWKXckYNYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                    Sairam Sairam Sairam"
      ],
      "metadata": {
        "id": "f7BptOAjvdDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}