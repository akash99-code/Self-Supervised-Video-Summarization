{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "22e5c399-9b5b-4ef2-a325-ec40d0c773b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438bc7c-8809-4140-9860-2ad047eed2e4",
   "metadata": {},
   "source": [
    "## Load COOT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf58a15c-8127-4511-9f78-d3b798259b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nntrainer.models.transformer_legacy import TransformerLegacy\n",
    "from nntrainer.models.transformer_legacy import TransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2ae81f-70a2-433e-9a3d-9d430e99a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_config = {'name': 'transformer', 'output_dim': 384, 'use_input_fc': True, 'input_fc_config': {'output_dim': 384, 'num_layers': 1, 'hidden_dim': 0, 'activation_middle': 'none', 'activation_output': 'gelu', 'dropout_middle': 0, 'dropout_output': 0, 'norm_middle': 'none', 'norm_output': 'none', 'residual': 'none'}, 'positional_encoding': 'sincos', 'add_local_cls_token': False, 'dropout_input': 0, 'norm_input': 'layernorm_coot', 'selfatn_config': {'hidden_dim': 384, 'num_layers': 1, 'num_heads': 8, 'pointwise_ff_dim': 384, 'activation': 'gelu', 'dropout': 0.05, 'norm': 'layernorm_coot'}, 'use_context': False, 'use_output_fc': False, 'pooler_config': {'name': 'atn', 'hidden_dim': 768, 'num_heads': 2, 'num_layers': 1, 'dropout': 0.05, 'activation': 'gelu'}, 'weight_init_type': 'truncnorm', 'weight_init_std': 0.01}\n",
    "current_cfg = TransformerConfig(transf_config)\n",
    "net_video_local = TransformerLegacy(current_cfg, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48110ec0-5603-42f4-81c5-3d5a8ea10f10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLegacy(\n",
       "  (norm_input): LayerNormalization()\n",
       "  (input_fc): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "    )\n",
       "    (activation_output): GELU(approximate='none')\n",
       "    (norm_output): Identity()\n",
       "  )\n",
       "  (embedding): PositionalEncodingSinCos(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (tf): TransformerEncoder(\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attention_layer): Sublayer(\n",
       "          (sublayer): MultiHeadAttention(\n",
       "            (query_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (final_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "            (softmax): Softmax(dim=3)\n",
       "          )\n",
       "          (layer_normalization): LayerNormalization()\n",
       "        )\n",
       "        (pointwise_feedforward_layer): Sublayer(\n",
       "          (sublayer): PointwiseFeedForwardNetwork(\n",
       "            (feed_forward): Sequential(\n",
       "              (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (1): Dropout(p=0.05, inplace=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_normalization): LayerNormalization()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): MultiGenPool(\n",
       "    (pools): ModuleList(\n",
       "      (0): GenPool(\n",
       "        pool linear torch.Size([2, 384, 384])\n",
       "        pool linear torch.Size([2, 384])\n",
       "        pool linear torch.Size([2, 384, 192])\n",
       "        pool linear torch.Size([2, 192])\n",
       "        (activation): GELU(approximate='none')\n",
       "        (dropout1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout3): Dropout(p=0.05, inplace=False)\n",
       "        (softmax): Softmax(dim=2)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_video_local.load_state_dict(torch.load('provided_models/Net_local.pth'))\n",
    "net_video_local.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8771f164-ae6d-41fa-ac9f-143d7b7b798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_config = {'name': 'transformer', 'output_dim': 768, 'use_input_fc': False, 'input_fc_config': None, 'use_context': True, 'crossatn_config': {'hidden_dim': 384, 'num_layers': 1, 'num_heads': 8, 'pointwise_ff_dim': 384, 'activation': 'gelu', 'dropout': 0.05, 'norm': 'layernorm_coot'}, 'pooler_config': {'name': 'avg_special'}, 'positional_encoding': 'sincos', 'add_local_cls_token': False, 'dropout_input': 0, 'norm_input': 'layernorm_coot', 'selfatn_config': {'hidden_dim': 384, 'num_layers': 1, 'num_heads': 8, 'pointwise_ff_dim': 384, 'activation': 'gelu', 'dropout': 0.05, 'norm': 'layernorm_coot'}, 'use_output_fc': False, 'weight_init_type': 'truncnorm', 'weight_init_std': 0.01}\n",
    "current_cfg = TransformerConfig(transf_config)\n",
    "coot = TransformerLegacy(current_cfg, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d958b200-7693-4daf-9b92-75c778bf07bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLegacy(\n",
       "  (norm_input): LayerNormalization()\n",
       "  (embedding): PositionalEncodingSinCos(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (tf): TransformerEncoder(\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attention_layer): Sublayer(\n",
       "          (sublayer): MultiHeadAttention(\n",
       "            (query_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (final_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "            (softmax): Softmax(dim=3)\n",
       "          )\n",
       "          (layer_normalization): LayerNormalization()\n",
       "        )\n",
       "        (pointwise_feedforward_layer): Sublayer(\n",
       "          (sublayer): PointwiseFeedForwardNetwork(\n",
       "            (feed_forward): Sequential(\n",
       "              (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (1): Dropout(p=0.05, inplace=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_normalization): LayerNormalization()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tf_context): TransformerDecoder(\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attention_layer): Sublayer(\n",
       "          (sublayer): MultiHeadAttention(\n",
       "            (query_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (final_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "            (softmax): Softmax(dim=3)\n",
       "          )\n",
       "          (layer_normalization): LayerNormalization()\n",
       "        )\n",
       "        (pointwise_feedforward_layer): Sublayer(\n",
       "          (sublayer): PointwiseFeedForwardNetwork(\n",
       "            (feed_forward): Sequential(\n",
       "              (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (1): Dropout(p=0.05, inplace=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_normalization): LayerNormalization()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): TemporalAvgPool()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coot.load_state_dict(torch.load('provided_models/TheModel.pth'))\n",
    "coot.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504bbf5-7f8f-4027-9389-376157b39abb",
   "metadata": {},
   "source": [
    "### Summarizer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4633f6-34da-48f5-b129-c34973a6fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knapsack_dp(values,weights,n_items,capacity,return_all=False):\n",
    "    # check_inputs(values,weights,n_items,capacity)\n",
    "    \n",
    "    # assert(isinstance(values,list))\n",
    "    # assert(isinstance(weights,list))\n",
    "    # assert(isinstance(n_items,int))\n",
    "    # assert(isinstance(capacity,int))\n",
    "    # check value type\n",
    "    # assert(all(isinstance(val,int) or isinstance(val,float) for val in values))\n",
    "    # assert(all(isinstance(val,int) for val in weights))\n",
    "    # check validity of value\n",
    "    # assert(all(val >= 0 for val in weights))\n",
    "    # assert(n_items > 0)\n",
    "    # assert(capacity > 0)\n",
    "\n",
    "    table = torch.zeros((n_items+1,capacity+1))\n",
    "    keep = torch.zeros((n_items+1,capacity+1))\n",
    "\n",
    "    for i in range(1,n_items+1):\n",
    "        for w in range(0,capacity+1):\n",
    "            wi = weights[i-1] # weight of current item\n",
    "            vi = values[i-1] # value of current item\n",
    "            if (wi <= w) and (vi + table[i-1,w-wi] > table[i-1,w]):\n",
    "                table[i,w] = vi*1 + table[i-1,w-wi]\n",
    "                keep[i,w] = 1\n",
    "            else:\n",
    "                table[i,w] = vi*0+table[i-1,w]\n",
    "    \n",
    "    \n",
    "    picks = []\n",
    "    cum_wghts = torch.zeros(n_items)\n",
    "    K = capacity\n",
    "    for i in range(n_items,0,-1):\n",
    "        if keep[i,K] == 1:\n",
    "            picks.append(i)\n",
    "            cum_wghts[i-1] = table[i,K]\n",
    "            K -= weights[i-1]\n",
    "\n",
    "    picks.sort()\n",
    "    picks = [x-1 for x in picks] # change to 0-index\n",
    "    chosens = (cum_wghts-torch.min(cum_wghts)) / (torch.max(cum_wghts)-torch.min(cum_wghts))\n",
    "    # print(picks, torch.ceil(chosens) )\n",
    "    \n",
    "\n",
    "    # if return_all:\n",
    "    #     max_val = table[n_items,capacity]\n",
    "    #     return picks,max_val\n",
    "    return picks, torch.ceil(chosens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39111375-c65f-4d71-99c8-8b61a871b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShotScore(nn.Module):\n",
    "    def __init__(self, input_size=384):\n",
    "        super(ShotScore, self).__init__()\n",
    "        self.Clip_Encoding = nn.Sequential(\n",
    "                                nn.TransformerEncoderLayer(d_model=input_size, nhead=8, batch_first=True),\n",
    "                                nn.TransformerEncoderLayer(d_model=input_size, nhead=8, batch_first=True))\n",
    "        self.Clip_Scoring = nn.Sequential(\n",
    "                                nn.Linear(in_features=input_size, out_features=input_size//2, bias=True),\n",
    "                                nn.Linear(in_features=input_size//2, out_features=1, bias=True))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self,clip_embs, stages,  batch_mask=None):\n",
    "        \n",
    "        f1 = clip_embs\n",
    "#         for s in range(stages):\n",
    "#             clip_encoder = self.Clip_Encoding(Features)+Features\n",
    "#             clip_Scores = self.Clip_Scoring(clip_encoder)\n",
    "\n",
    "#     #         Scene_emb_list = []\n",
    "#     #         for start, end in scene_boundaries:\n",
    "#     #             scene_embs = self.scene_embs(clip_embs[start:end])\n",
    "#     #             Scene_emb_list.append(scene_embs)\n",
    "\n",
    "#     #             Scene_Scores = self.Scene_Scoring_Transformer(Scene_emb_list)\n",
    "\n",
    "#     #         for start, end in scene_boundaries:  \n",
    "#                 # clip_Scores+=Scene_Scores\n",
    "\n",
    "#             si = self.softmax(clip_Scores)\n",
    "#             progressive_scores.append(si)\n",
    "#             Features += si*Features\n",
    "        \n",
    "        s1 = self.softmax(self.Clip_Scoring(self.Clip_Encoding(f1)+f1))\n",
    "        \n",
    "        f2 = s1*f1 + f1\n",
    "        s2 = self.softmax(self.Clip_Scoring(self.Clip_Encoding(f2)+f2))\n",
    "        \n",
    "        f3 = s2*f2 + f2\n",
    "        s3 = self.softmax(self.Clip_Scoring(self.Clip_Encoding(f3)+f3))\n",
    "            \n",
    "        \n",
    "        scores = self.softmax(s1*s2*s3)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352fa074-4eab-47b1-8b98-0c2215545d33",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04734675-da9d-4cc7-9d26-55cda41ce795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname == 'Linear':\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=np.sqrt(2.0))\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4bf1cd4-b12c-42b8-8b99-01d349a6a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_context(key, pred_clip_mask, nfps):\n",
    "    with h5py.File('data/youcook2/video_feat_100m.h5', 'r') as d:\n",
    "        vid_feats = torch.tensor(d[key][()])\n",
    "        mask = torch.zeros(len(vid_feats))\n",
    "        \n",
    "        # start = 0\n",
    "        # for i in range(len(nfps)):\n",
    "        #     end = nfps[i]+start\n",
    "        #     if i in picks:\n",
    "        #         mask[start:end]=1\n",
    "        #     start = end\n",
    "        \n",
    "        start = 0\n",
    "        for i in range(len(nfps)):\n",
    "            end = nfps[i]+start\n",
    "            mask[start:end]=pred_clip_mask[i]\n",
    "            start = end\n",
    "            \n",
    "        masked_vidfeats = vid_feats*mask.unsqueeze(0).T\n",
    "        masked_vidfeats = masked_vidfeats.unsqueeze(0)\n",
    "        masked_vidfeats = masked_vidfeats[masked_vidfeats.sum(dim=2) != 0].unsqueeze(0)\n",
    "        nfs = mask.sum().int()\n",
    "        mask = torch.zeros(1, nfs).bool()\n",
    "        vid_context,_ = net_video_local(masked_vidfeats, mask, nfs, None)\n",
    "        return vid_context\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c52db215-db1f-4a5a-bb3a-558b46ac7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = torch.nn.CosineSimilarity(dim=0, eps=0)\n",
    "def diversity_loss(masked_vid):\n",
    "    agg = []\n",
    "    dl = 0\n",
    "    for i in range(masked_vid.shape[0]):\n",
    "        for j in range(i+1, masked_vid.shape[0]):\n",
    "            yi = masked_vid[i]\n",
    "            yj = masked_vid[j]\n",
    "            cs = cos_sim(yi, yj)\n",
    "            agg.append(cs)\n",
    "            dl+=cs\n",
    "            \n",
    "    return torch.mean(torch.stack(agg))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b7d4a9a0-eff1-4f9f-9e21-ebb6becace55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_size=1, device='cpu'):\n",
    "    with open('frms_per_seg.json', 'r') as file:\n",
    "        NFPS = json.load(file)\n",
    "    l = nn.MSELoss()\n",
    "    ScoreModel = model\n",
    "    parameters = filter(lambda p: p.requires_grad, ScoreModel.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.00005, weight_decay=0.00001).to(device)\n",
    "    prev_mask=0\n",
    "    with h5py.File('experiments/retrieval/paper2020/yc2_100m_coot_valset1/embeddings/embeddings_0.h5', 'r') as d:\n",
    "        clips = d['clip_num'][...]\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            pointer = 0\n",
    "            epoch_loss = []\n",
    "            \n",
    "            print('Epoch:', epoch,'/50')\n",
    "            pbar = tqdm(total=len(clips), position=0, leave=True)\n",
    "            for clip_num, nclips in enumerate(clips):\n",
    "                key = d['key'][clip_num].decode()\n",
    "                # print(key)\n",
    "                clip_embs = d['clip_emb'][pointer:pointer+nclips]\n",
    "                ori_vid_cont = d['vid_context'][clip_num]\n",
    "                ori_vid_emb = d['vid_emb'][clip_num]\n",
    "                pointer += nclips\n",
    "\n",
    "                # vid_cont = torch.tensor(vid_context).unsqueeze(0)\n",
    "                clip_embs = torch.tensor(clip_embs).unsqueeze(0).to(device)\n",
    "                ori_vid_cont = torch.tensor(ori_vid_cont).unsqueeze(0)\n",
    "                ori_vid_emb = torch.tensor(ori_vid_emb).unsqueeze(0)\n",
    "\n",
    "\n",
    "                scores = ScoreModel(clip_embs,3).reshape(-1)\n",
    "\n",
    "                nfps = NFPS[key]\n",
    "\n",
    "                picks, predicted_mask = knapsack_dp(scores.squeeze(0), NFPS[key], len(nfps), math.ceil(sum(nfps)*15/100))\n",
    "\n",
    "\n",
    "                # print('###')\n",
    "                # predicted_mask = torch.zeros(nclips)\n",
    "                # predicted_mask[picks]=1\n",
    "\n",
    "                # mask = torch.ceil(predicted_mask.unsqueeze(0)*scores)\n",
    "                mask = predicted_mask.unsqueeze(0)\n",
    "\n",
    "\n",
    "                # if clip_num==0:\n",
    "                #     print(mask-prev_mask)\n",
    "                #     prev_mask=mask\n",
    "                    \n",
    "                masked_vid = clip_embs*mask.T\n",
    "\n",
    "                masked_vid = masked_vid[masked_vid.sum(dim=2) != 0].unsqueeze(0)\n",
    "\n",
    "                ### Check masked_vid has grads\n",
    "                clipn = predicted_mask.sum().int()\n",
    "                mask = torch.zeros(1, clipn).bool()\n",
    "                \n",
    "                vid_cont = get_hidden_context(key, predicted_mask, nfps)\n",
    "\n",
    "                \n",
    "                vid_emb, _ = coot(masked_vid, mask, clipn, vid_cont)\n",
    "                # print(ori_vid_emb.shape)\n",
    "                # print(vid_emb.shape)\n",
    "\n",
    "                div_loss = diversity_loss(masked_vid.squeeze())\n",
    "                reconstruction_loss = l(vid_emb, ori_vid_emb)\n",
    "                context_loss  = l(vid_cont, ori_vid_cont)\n",
    "                Loss = (0.2*div_loss) + (0.5*reconstruction_loss) + (0.3*context_loss)\n",
    "\n",
    "                # print(\"Loss:\", Loss)\n",
    "                optimizer.zero_grad()\n",
    "                Loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss.append(Loss.item())\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_description(\"Loss= %.3f\" % Loss.item())\n",
    "            pbar.close()\n",
    "                \n",
    "            print('Average Epoch Loss:', sum(epoch_loss)/len(epoch_loss))\n",
    "            torch.save(ScoreModel.state_dict(), f'models/model@{epoch%5}.pth')\n",
    "            \n",
    "            if(epoch%5==0):\n",
    "                fs_gt, fs_us = validation(ScoreModel)\n",
    "                print('Avg GT FSore:',fs_gt, '   Avg User FSore:', fs_us)\n",
    "            \n",
    "            \n",
    "    return(ScoreModel)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac224a-b17f-4703-878a-463531b1cc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb7d6052-db74-4f18-9e6b-2a14ea10cc99",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1411cbdd-b570-4c28-adc5-e613e5c6065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(ScoreModel):\n",
    "    \n",
    "    with open('frms_per_seg.json', 'r') as file:\n",
    "        NFPS = json.load(file)\n",
    "        \n",
    "    avg_gtfs = []\n",
    "    avg_usfs = []\n",
    "    with h5py.File('experiments/retrieval/paper2020/yc2_100m_coot_valset1/embeddings/embeddings_0.h5', 'r') as d, h5py.File('summaries.h5','r') as summh5:\n",
    "        clips = d['clip_num'][...]\n",
    "\n",
    "        pointer = 0\n",
    "        for clip_num, nclips in enumerate(clips):\n",
    "            key = d['key'][clip_num].decode()\n",
    "            clip_embs = d['clip_emb'][pointer:pointer+nclips]\n",
    "            pointer += nclips\n",
    "\n",
    "            clip_embs = torch.tensor(clip_embs).unsqueeze(0)\n",
    "\n",
    "            scores = ScoreModel(clip_embs, 3).reshape(-1)\n",
    "\n",
    "            nfps = NFPS[key]\n",
    "\n",
    "            picks,_ = knapsack_dp(scores.squeeze(0), NFPS[key], len(nfps), math.ceil(sum(nfps)*15/100))\n",
    "            \n",
    "            machine_summary = get_machine_summ(picks, nfps)\n",
    "            gt_summary = summh5[key]['gt_summary']\n",
    "            user_summary = summh5[key]['user_summary']\n",
    "            fs_gt,_,_ = evaluate_summary(machine_summary, np.expand_dims(gt_summary, 0) )\n",
    "            fs_ut,_,_ = evaluate_summary(machine_summary, np.array(user_summary) )\n",
    "            avg_gtfs.append(fs_gt)\n",
    "            avg_usfs.append(avg_usfs)\n",
    "            \n",
    "        return fs_gt.mean(), fs_ut.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6efc7d7e-edb4-4e24-ae59-e5f5cd04bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_machine_summ(picks, nfps):\n",
    "    tot_feats = sum(nfps)\n",
    "    summ = []\n",
    "    for n, nfs in enumerate(nfps):\n",
    "        if n in picks:\n",
    "            summ = np.append(summ, np.ones(16*nfs))\n",
    "        else:\n",
    "            summ = np.append(summ, np.zeros(16*nfs))\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3f5f80ac-e798-4c73-aa0b-106162b482f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary(machine_summary, user_summary, eval_metric='avg'):\n",
    "    \"\"\"Compare machine summary with user summary (keyshot-based).\n",
    "    Args:\n",
    "    --------------------------------\n",
    "    machine_summary and user_summary should be binary vectors of ndarray type.\n",
    "    eval_metric = {'avg', 'max'}\n",
    "    'avg' averages results of comparing multiple human summaries.\n",
    "    'max' takes the maximum (best) out of multiple comparisons.\n",
    "    \"\"\"\n",
    "    machine_summary = machine_summary.astype(np.float32)\n",
    "    user_summary = user_summary.astype(np.float32)\n",
    "    n_users,n_frames = user_summary.shape\n",
    "\n",
    "    # binarization\n",
    "    machine_summary[machine_summary > 0] = 1\n",
    "    user_summary[user_summary > 0] = 1\n",
    "\n",
    "    if len(machine_summary) > n_frames:\n",
    "        machine_summary = machine_summary[:n_frames]\n",
    "    elif len(machine_summary) < n_frames:\n",
    "        zero_padding = np.zeros((n_frames - len(machine_summary)))\n",
    "        machine_summary = np.concatenate([machine_summary, zero_padding])\n",
    "\n",
    "    f_scores = []\n",
    "    prec_arr = []\n",
    "    rec_arr = []\n",
    "\n",
    "    for user_idx in range(n_users):\n",
    "        gt_summary = user_summary[user_idx,:]\n",
    "        overlap_duration = (machine_summary * gt_summary).sum()\n",
    "        precision = overlap_duration / (machine_summary.sum() + 1e-8)\n",
    "        recall = overlap_duration / (gt_summary.sum() + 1e-8)\n",
    "        if precision == 0 and recall == 0:\n",
    "            f_score = 0.\n",
    "        else:\n",
    "            f_score = (2 * precision * recall) / (precision + recall)\n",
    "        f_scores.append(f_score)\n",
    "        prec_arr.append(precision)\n",
    "        rec_arr.append(recall)\n",
    "\n",
    "    final_f_score = np.mean(f_scores)\n",
    "    final_prec = np.mean(prec_arr)\n",
    "    final_rec = np.mean(rec_arr)\n",
    "    return final_f_score, final_prec, final_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f7802-03c4-417c-aff7-fa1b373a2584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6587c-f1cb-412c-94d8-726a482f558d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a9c8800-ef28-46aa-921f-1a1a35482010",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "777c59b2-ed9b-412b-a4e8-1e3156aab11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 /50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss= 0.834:   7%|▋         | 5/72 [00:04<00:51,  1.31it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [157]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(weights_init)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# model.to('gpu')\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [155]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, batch_size)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(\"Loss:\", Loss)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 73\u001b[0m \u001b[43mLoss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     75\u001b[0m epoch_loss\u001b[38;5;241m.\u001b[39mappend(Loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ShotScore()\n",
    "# model.load_state_dict(torch.load('model/model@i.pth', map_location=lambda storage, loc: storage))\n",
    "model.apply(weights_init)\n",
    "# model.to('gpu')\n",
    "trained_model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc75241-042a-43de-ac41-7b3454ba714a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.tensor([2.2,3.6])\n",
    "x.sum().int().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7c82cb-e8ad-47be-a61a-1ffedbd70ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5, dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(5).int()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
